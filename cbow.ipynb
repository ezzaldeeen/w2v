{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample corpus\n",
    "corpus = \"\"\"\n",
    "The cat and her kittens, they put on their mittens,\n",
    "To eat a Christmas pie. The cat and her kittens, they put on their mittens,\n",
    "To eat a Christmas pie. The cat and her kittens, they put on their mittens.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 15\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # convert to lower case\n",
    "    corpus = corpus.lower()\n",
    "    # remove punctuation\n",
    "    corpus = corpus.replace(',', '')\n",
    "    corpus = corpus.replace('.', '')\n",
    "    # split into words\n",
    "    words = corpus.split()\n",
    "    return words\n",
    "\n",
    "# tokenize the corpus\n",
    "tokens = tokenize(corpus)\n",
    "# vocabulary (set of unique words)\n",
    "vocab = set(tokens)\n",
    "# vocabulary size\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "id2token = {i: t for i, t in enumerate(vocab)}\n",
    "token2id = {t: i for i, t in enumerate(vocab)}\n",
    "\n",
    "def encode(token, token2id):\n",
    "    # create a one-hot vector\n",
    "    idx = token2id[token]\n",
    "    vector = [0] * vocab_size\n",
    "    vector[idx] = 1\n",
    "    return np.array(vector)\n",
    "\n",
    "\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "    objective:\n",
    "        based on the given context try to predict the center word,\n",
    "        training the neural net on this objective going to build\n",
    "        a word representaion (word embeddings) hopefully.\n",
    "\n",
    "    preparing the training data:\n",
    "        [0, 0, 1, 0, ..., 0] -> 'context word(i-2)'\n",
    "        [0, 1, 0, 0, ..., 0] -> 'context word(i-1)'\n",
    "        [1, 0, 0, 0, ..., 0] -> 'context word(i+1)'\n",
    "        [0, 0, 0, 0, ..., 1] -> 'context word(i+2)'\n",
    "\n",
    "        compined vectors:\n",
    "        [1, 1, 1, 1, ..., 1] -> 'context words' # this is going to be the input\n",
    "\n",
    "        center word:\n",
    "        [0, 0, 0, 0, ..., 0] -> 'center word' # to be predicted\n",
    "\n",
    "        NOTE: values in vectors just for demo\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(tokens, window_size=2):\n",
    "    # training data consists of \n",
    "    # context words and center words\n",
    "    # where the model learns to predict\n",
    "    # the center word from the context words\n",
    "    contexts = []\n",
    "    centers = []\n",
    "    for i in range(len(tokens)):\n",
    "        # context per current center word\n",
    "        # to save the context words and combine them\n",
    "        # into single vector per center word\n",
    "        context = []\n",
    "        for j in range(max(0, i-window_size), min(len(tokens), i+window_size+1)):\n",
    "            if i != j:\n",
    "                # encode context word\n",
    "                context.append(encode(tokens[j], token2id))\n",
    "        # encode center word\n",
    "        centers.append(encode(tokens[i], token2id))\n",
    "        # combine context words into single vector\n",
    "        contexts.append(np.sum(context, axis=0))\n",
    "        \n",
    "    return np.array(contexts), np.array(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts: (40, 15)\n",
      "centers: (40, 15)\n"
     ]
    }
   ],
   "source": [
    "# build the training data\n",
    "contexts, centers = build_data(tokens)\n",
    "print(\"contexts:\", contexts.shape)\n",
    "print(\"centers:\", centers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel:\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        # initialize the weights\n",
    "        self.w1 = np.random.randn(vocab_size, embedding_size)\n",
    "        self.w2 = np.random.randn(embedding_size, vocab_size)\n",
    "        assert self.w1.shape[1] == self.w2.shape[0]\n",
    "\n",
    "    def train(self, contexts, centers, epochs=3, lr=0.025):\n",
    "        pass\n",
    "\n",
    "    def _forward(self, contexts, centers):\n",
    "        assert contexts.shape[1] == self.w1.shape[0]\n",
    "        self.a1 = np.dot(contexts, self.w1) # out shape: (N, embed_size)\n",
    "        self.a2 = np.dot(self.a1, self.w2) # out shape: (N, vocab)\n",
    "        self.z = self._softmax(self.a2)\n",
    "\n",
    "    def _backward(self, lr):\n",
    "        pass\n",
    "\n",
    "    def _softmax(self, logits):\n",
    "        pass\n",
    "\n",
    "    def _cross_entropy(self, actual, prediction):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
