{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e967b2-3236-4be9-8a09-7fc2880bc6f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d0f7e1-b535-4c82-872a-3c17c5035bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the 100th Etext file presented by Project Gutenberg, and\\nis presented in cooperation with World Library, Inc., from their\\nLibrary of the Future and Shakespeare CDROMS.  Project Gutenberg\\noften releases Etexts that are NOT placed in the Public Domain!!\\n\\nShakespeare\\n\\n*This Etext has certain copyright implications you should read!*\\n\\n<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM\\nSHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS\\nPROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE\\nWITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE\\nDISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS\\nPERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED\\nCOMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY\\nSERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>\\n\\n*Project Gutenberg is proud to cooperate with The World Library*\\nin the presentation of The Complete Works of William Shakespeare\\nfor your reading for educatio'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./shakespeare.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "    \n",
    "sample = content[:1000]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9feaa9-2c4b-452a-949d-98d0f53a77d6",
   "metadata": {},
   "source": [
    "---\n",
    "    training data should look like:\n",
    "    \n",
    "    <center>       ->    <context>\n",
    "    [0, ..., 1, 0] ->    [0, ..., 1, 0]\n",
    "    [0, ..., 1, 0] ->    [0, ..., 0, 1]\n",
    "    [0, ..., 1, 0] ->    [1, ..., 0, 0]\n",
    "    ...            ->    ...\n",
    "    \n",
    "    where the center word (input) have a number of <context words>\n",
    "    based on the size of the window hence:\n",
    "        if window_size = 2; two context words from both sides (left & right).\n",
    "    \n",
    "    objective:\n",
    "        based on the given input <center word> try to predict the output <context word>\n",
    "        and when we reach to a good performance we hope that the neural network will\n",
    "        try to extract hiddin features, which are going to be the meanings of input words\n",
    "        in a numerical representation.\n",
    "        \n",
    "    steps:\n",
    "        1. split input text into chunks (tokens)\n",
    "        2. assign a token_id per token\n",
    "        3. encode thoes tokens based on their ids (one-hot encoding)\n",
    "        4. building the training data as intended above\n",
    "        5. \n",
    "    \n",
    "    NOTE: this is going to be the basic solution and from here i'll try to improve.\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff3d56c3-fa63-4df1-89dd-bfdf1a8dc2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # simple tokenizer (not the best)\n",
    "    return text.lower().split(' ')\n",
    "\n",
    "def encode(token_id, vocab_size):\n",
    "    vector = [0] * vocab_size  # which is going to be the input vector size\n",
    "    vector[token_id] = 1\n",
    "    return np.array(vector)  # for training (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba773985-37d7-45e4-b85f-f78a74a8e12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'the', '100th', 'etext', 'file', 'presented', 'by']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize(sample)\n",
    "tokens[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba48f6d-6e4f-4e40-b72e-230bde5f6faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {t: i for i, t in enumerate(set(tokens))}\n",
    "id_to_token = {i: t for i, t in enumerate(set(tokens))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e1c644c-0f01-4ed7-898c-1822e3ee3b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(set(tokens))\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1978f1f-d8bd-47c2-9bd9-b62dc92f460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(*iterables):\n",
    "    # to combine two iterables\n",
    "    # into single iterable\n",
    "    for iterable in iterables:\n",
    "        yield from iterable\n",
    "\n",
    "def build_train(tokens, token_to_id, vocab_size, window_size=2, verbose=False):\n",
    "    center_vectors = []\n",
    "    context_vectors = []\n",
    "    for i, center in enumerate(tokens):\n",
    "        # context words iterable on the right side\n",
    "        r_ctx = range(i+1, min(i+window_size+1, len(tokens)))\n",
    "        # context words iterable on the left side\n",
    "        l_ctx = range(max(0, i-window_size), i)\n",
    "        # combined context iterables from both sides\n",
    "        c_ctx = combine(l_ctx, r_ctx)\n",
    "    \n",
    "        # get numerical representation (one-hot encoding)\n",
    "        # for both: 1. center word 2. context word    \n",
    "        cnt_w_id = token_to_id[center]  \n",
    "        cnt_w_vector = encode(cnt_w_id, vocab_size)  # 1. center word\n",
    "        for w_id in c_ctx:\n",
    "            if verbose:\n",
    "                # <center word>  ->  <ctx word>\n",
    "                print(f\"{center} -> {tokens[w_id]}\")\n",
    "                \n",
    "            ctx_w_id = token_to_id[tokens[w_id]]\n",
    "            ctx_w_vector = encode(ctx_w_id, vocab_size)  # 2. context word\n",
    "            \n",
    "            center_vectors.append(cnt_w_vector)\n",
    "            context_vectors.append(ctx_w_vector)\n",
    "    # store them as numpy.array for training        \n",
    "    return np.array(center_vectors), np.array(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "512fe169-a6ae-49b3-857b-7ca1dd645a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center words shape:  (546, 90)\n",
      "context words shape:  (546, 90)\n"
     ]
    }
   ],
   "source": [
    "cnt, ctx = build_train(tokens[:1000], token_to_id, vocab_size)\n",
    "print(\"center words shape: \", cnt.shape)\n",
    "print(\"context words shape: \", ctx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6ba3c-3e28-47e5-986e-774759b34ec1",
   "metadata": {},
   "source": [
    "    center words are duplicated:\n",
    "    where the same center word has more than context words\n",
    "    the size depends on the `window_size`\n",
    "    \n",
    "    center word (1) -> context word (1)\n",
    "                    -> context word (2)\n",
    "                    -> ...\n",
    "                    -> context word (N)\n",
    "    ...\n",
    "    center word (M) -> context word (1)\n",
    "                    -> context word (2)\n",
    "                    -> ...\n",
    "                    -> context word (N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b094673-0f20-4a2a-bdb9-98e0e18142ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "    Forward pass:\n",
    "    -------------\n",
    "    A1   = X @ W1\n",
    "    A2   = A1 @ W2\n",
    "    Z    = softmax(A2)\n",
    "    Loss = cross_entropy(Z, y)\n",
    "    \n",
    "    \n",
    "    How changing W2 impacted the loss?\n",
    "    ----------------------------------\n",
    "    >> dLoss(Z, y)/dW2 = dLoss/dZ * dZ/dA2 * dA2/dW2 \n",
    "    >> dLoss(Z, y)/dW2 = (z - y)           * A1\n",
    "    shape convention:\n",
    "    - (z - y): (#samples, vocab_size)\n",
    "    - A1:      (#samples, embedding_size)\n",
    "    \n",
    "    shape of target matrix to update (W2): (embedding_size, vocab_size)\n",
    "    \n",
    "    ==> A1.T @ (z-y)\n",
    "    \n",
    "    \n",
    "    How changing W1 impacted the loss?\n",
    "    ----------------------------------\n",
    "    >> dLoss(Z, t)/dW1 = dLoss/dZ * dZ/dA2 * dA2/dA1 * dA1/dW1\n",
    "    >> dLoss(Z, t)/dW1 = (z - y)           * W2      * X\n",
    "    \n",
    "    shape convention:\n",
    "    - (z - y): (#samples, vocab_size)\n",
    "    - W2:      (embedding_size, vocab_size)\n",
    "    - X:       (#samples, vocab_size) \n",
    "    \n",
    "    shape of target matrix to update (W1): (vocab_size, embedding_size)\n",
    "    \n",
    "    ==> x.T @ ( (z-y) @ W2.T )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf33ae72-1644-433c-8811-3fda97af8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, X, y, vocab_size, embedding_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.vocab_size = vocab_size\n",
    "        self.w1 = np.random.rand(vocab_size, embedding_size)\n",
    "        self.w2 = np.random.rand(embedding_size, vocab_size)\n",
    "    \n",
    "    def forward(self):\n",
    "        self.a1 = self.X @ self.w1\n",
    "        self.a2 = self.a1 @ self.w2\n",
    "        self.z = self._softmax(self.a2)\n",
    "        return self.z\n",
    "    \n",
    "    def backward(self, alpha):\n",
    "        self.forward()\n",
    "        # derivatives\n",
    "        dl = self.z - self.y\n",
    "        dw2 = self.a1.T @ dl\n",
    "        dw1 = self.X.T @ (dl @ self.w2.T)\n",
    "        # update weights\n",
    "        self.w2 = self.w2 - (alpha * dw2)\n",
    "        self.w1 = self.w1 - (alpha * dw1)\n",
    "        # compute loss\n",
    "        return self._cross_entropy(self.y, self.z)\n",
    "    \n",
    "    @staticmethod    \n",
    "    def _softmax(a):\n",
    "        a = a - np.max(a)\n",
    "        return np.exp(a) / np.sum(np.exp(a))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _cross_entropy(actual, predicted):\n",
    "        return - np.sum(actual * np.log(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bd04e0c-8ee0-4489-b0f1-c11ecb94d9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(cnt, ctx, vocab_size, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4190bd4-1911-45bb-a80a-f2de0f9ed01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6019.210012276704"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backward(alpha=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f83e21-c701-4a10-a381-10a6bf4fe39d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
